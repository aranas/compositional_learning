{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns \n",
    "current = os.path.dirname(os.path.realpath('plotting.py'))\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "import functions.plotting as NNplt\n",
    "from functions.rnn_cryptic_equals import generate_sequences, convert_seq2inputs,calculate_output, onehot2seq \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1c75e",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7103e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(OneStepRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers= num_layers,\n",
    "                        batch_first=True)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define the layers\n",
    "        self.input2hidden = nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        self.fc1tooutput = nn.Linear(self.hidden_size, output_size)\n",
    "        self.initialize_weights()\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), dim=0) ## dim = 1??\n",
    "        self.hidden = nn.functional.relu(self.input2hidden(combined))\n",
    "        self.output = self.fc1tooutput(self.hidden)\n",
    "        #return self.output.view(-1,output_size), self.hidden\n",
    "        return self.output, self.hidden\n",
    "\n",
    "    def get_activations(self, x, hidden):\n",
    "        self.forward(x, hidden)  # update the activations with the particular input\n",
    "        return self.hidden, self.output #, self.fc1_activations\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.hidden_noise\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)[0]\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        y = 1/math.sqrt(hidden_size)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, 0.000001)\n",
    "\n",
    "\n",
    "def train(sequence, label ,model ,optimizer ,criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #Read each cue in and keep hidden state for next cue\n",
    "    hidden = model.initHidden()\n",
    "    batch_out = []\n",
    "    for batchseq in sequence:\n",
    "        for i in range(len(batchseq)):\n",
    "            output, hidden = model.forward(batchseq[i], hidden)\n",
    "        batch_out.append(output)\n",
    "        #Compare final output to target\n",
    "    batch_out = torch.cat(batch_out)\n",
    "    loss = criterion(batch_out,label)#.long())\n",
    "\n",
    "    #Back-propagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return batch_out, loss.item()\n",
    "\n",
    "def run_acc(model,optimizer,criterion, train_data, test_data, epochs, verbose = False):\n",
    "    \n",
    "    loss_history = np.empty((0,1))\n",
    "    all_accs = np.empty((0,len(test_data)))\n",
    "    for epoch in range(epochs):\n",
    "        lossTotal = 0\n",
    "        for i, (x,y) in enumerate(train_data):\n",
    "            output, loss = train(x,y,model,optimizer,criterion)\n",
    "            lossTotal += loss # add MSE -> sum of square errors \n",
    "        loss_history = np.vstack([loss_history, lossTotal])\n",
    "        acc = test_acc(model, test_data, hidden_size)\n",
    "        all_accs = np.vstack([all_accs,acc])\n",
    "\n",
    "    return loss_history, all_accs\n",
    "\n",
    "def test_acc(model, testdata, hidden_size, verbose = False):\n",
    "    model.eval()\n",
    "    accs = np.empty((1, 0))\n",
    "    for testset in testdata:\n",
    "        batch_correct = []\n",
    "        for x,y in testset:\n",
    "            correct = 0\n",
    "            for i in range(len(x)):\n",
    "                hidden = torch.zeros(1, hidden_size)[0]\n",
    "                for step in x[i]:\n",
    "                    hidden, y_hat = model.get_activations(step,hidden)\n",
    "                correct += sum(torch.round(y[i]) == torch.round(y_hat)).item()\n",
    "            batch_correct.append(correct/len(y))\n",
    "        acc = np.mean(batch_correct)\n",
    "        accs = np.append(accs, [acc])\n",
    "    if verbose:\n",
    "        print('test accuracy: %f ' % (acc))\n",
    "    return accs\n",
    "\n",
    "def test_preds(model, testdata, hidden_size, suffix = ''):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labs = []\n",
    "    trials = []\n",
    "    accs = []\n",
    "    for testset in testdata:\n",
    "        batch_correct = []\n",
    "        for x,y in testset:\n",
    "            for i in range(len(x)):\n",
    "                hidden = torch.zeros(1, hidden_size)[0]\n",
    "                for step in x[i]:\n",
    "                    hidden, y_hat = model.get_activations(step,hidden)\n",
    "                preds.append(y_hat.detach().item())\n",
    "                labs.append(y[i].detach().item())\n",
    "                correct = sum(torch.round(y[i]) == torch.round(y_hat)).item()\n",
    "                accs.append(correct)\n",
    "            trials.append(str(onehot2seq(x)))\n",
    "    df = pd.DataFrame({'trial':trials, 'label'+suffix:labs, 'pred'+suffix: preds, 'acc'+suffix: accs})\n",
    "    return df \n",
    "\n",
    "\n",
    "def shuffle_weights(model):\n",
    "    model2 = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    mod_dict = model.state_dict()\n",
    "    shuffled_dict = {layer: shuffle_tensor(val) for layer, val in mod_dict.items()}\n",
    "    model2.load_state_dict(shuffled_dict)\n",
    "    return model2\n",
    "\n",
    "def shuffle_tensor(t):\n",
    "    idx = torch.randperm(t.nelement())\n",
    "    t = t.view(-1)[idx].view(t.size())\n",
    "    return t\n",
    "\n",
    "def run_sim(train_trials, test_trials):\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, train_trials, test_trials, epochs)\n",
    "    return loss1, acc1, model\n",
    "\n",
    "def run_sims(i, train_trials, test_trials):\n",
    "    print('##########Â rep', i, '#########')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, train_trials[0], test_trials, epochs)\n",
    "    loss2, acc2 = run_acc(model,optimizer,criterion, train_trials[1], test_trials, epochs)\n",
    "    losses = np.vstack([loss1,loss2])\n",
    "    accs = np.vstack([acc1,acc2])\n",
    "    return losses, accs, model\n",
    "\n",
    "\n",
    "def change_dict(seqs, new_dict):\n",
    "    \"\"\" recalculates sequence output\"\"\"\n",
    "    inps = [s[:-1] for s in seqs]\n",
    "    for inp in inps:\n",
    "        inp.append(calculate_output(inp, new_dict))\n",
    "\n",
    "    return inps\n",
    "\n",
    "def predcorr(mods, tests):\n",
    "    dfs1 = []\n",
    "    for i in range(len(mods)):\n",
    "        \n",
    "        df = test_preds(mods[i], [tests[i]], hidden_size)\n",
    "        dfs1.append(df)\n",
    "        all_dfs1 = pd.concat(dfs1) \n",
    "        preds, labs = all_dfs1['pred'], all_dfs1['label']\n",
    "        xy = np.arange(np.min(preds)-1, np.max(labs)+1, 0.1)\n",
    "        r2_val = r2_score(all_dfs1['pred'],all_dfs1['label'])\n",
    "        \n",
    "        for d in dfs1:\n",
    "            plt.scatter(d['label'], d['pred'])\n",
    "        plt.plot(xy,xy)\n",
    "        plt.xlabel('Ground truth')\n",
    "        plt.ylabel('Model prediction')\n",
    "        plt.title('with primitive training, R^2 = ' + str(round(r2_val, 2)) )\n",
    "        \n",
    "        print(all_dfs1.groupby(['trial']).mean().sort_values(by = 'acc' , ascending=False))\n",
    "    return r2_val, all_dfs1.groupby(['trial']).mean().sort_values(by = 'acc' , ascending=False) \n",
    "\n",
    "# -----------------\n",
    "# Generating trials\n",
    "# -----------------\n",
    "\n",
    "def generate_primitives(inputs):\n",
    "    seq = []\n",
    "    for inp in inputs:\n",
    "        trial = [inp, '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def generate_trials(operators, input_ids, init_values):\n",
    "    \n",
    "    ''' function for generating all permutations of 1 step trials '''\n",
    "    \n",
    "    seq = []\n",
    "    combi_operators = list(itertools.product(operators, repeat=1))\n",
    "    combi_inputcue = list(itertools.product(input_ids, repeat=1))\n",
    "    for init in init_values:\n",
    "        for cue in combi_inputcue:\n",
    "            for op in combi_operators:\n",
    "                seq.append([init,\n",
    "                            *zip(tuple(op), cue), '=']) #group per time point t\n",
    "    for s in seq:\n",
    "        s.append(calculate_output(s, cue_dict))\n",
    "    return seq\n",
    "\n",
    "def generate_self(operators, inputs):\n",
    "    seq = []\n",
    "    for inp in inputs:\n",
    "        trial = generate_trials(operators, inp, inp)\n",
    "        seq += trial\n",
    "    return seq\n",
    "\n",
    "def generate_other_pairs(op, inputs):\n",
    "    seq = []\n",
    "    inputs1 = inputs.copy()\n",
    "    inputs2 = inputs.copy()\n",
    "    inputs2.append(inputs2.pop(0))\n",
    "    for i in range(len(inputs1)):\n",
    "        trial = [inputs1[i], (op, inputs2[i]), '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "\n",
    "# ----------\n",
    "# plotting\n",
    "# ----------\n",
    "\n",
    "def plotNNs(loss_arrays, acc_arrays, labels, colors, title, shift = 0):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    loss_cols = ['blue', 'darkblue']\n",
    "    loss_labs = ['loss_with_primitive', 'loss_without_primitive']\n",
    "    for i, arr in enumerate(loss_arrays):\n",
    "        x = np.arange(0,arr.shape[0],1) + shift\n",
    "        mn = arr.mean(axis=1)\n",
    "        errs = arr.std(axis=1)\n",
    "        \n",
    "        axs[0].plot(x, mn, label = loss_labs[i], color = loss_cols[i])\n",
    "        axs[0].fill_between(x, mn - errs, mn + errs, alpha = 0.3, facecolor = loss_cols[i])\n",
    "    \n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    for i, arr in enumerate(acc_arrays):\n",
    "        x = np.arange(0,arr.shape[0],1) + shift\n",
    "        mn = arr.mean(axis=1)\n",
    "        errs = arr.std(axis=1)\n",
    "        \n",
    "        axs[1].plot(x, mn, label = labels[i], color = colors[i])\n",
    "        axs[1].fill_between(x, mn - errs, mn + errs, alpha = 0.3, facecolor = colors[i])\n",
    "    \n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.suptitle(title, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fd65d",
   "metadata": {},
   "source": [
    "# RNN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d49005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "num_classes = 22\n",
    "batchsize=1\n",
    "\n",
    "# RNN specs\n",
    "input_size = num_classes\n",
    "output_size = 1\n",
    "num_layers = 1\n",
    "hidden_size = 20\n",
    "learningRate = 0.005\n",
    "\n",
    "## running params\n",
    "epochs = 1000\n",
    "num_sims = 8\n",
    "num_inputs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e5b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running function\n",
    "\n",
    "def run_exp(trainseqs_old, testseqs_old):\n",
    "\n",
    "    all_syms = list(cue_dict.keys())[1:]\n",
    "    all_input_vals = list(np.arange(2,18))\n",
    "    input_vals = random.sample(all_input_vals,num_inputs)\n",
    "    #Â randomly select values for each input\n",
    "    for i, s in enumerate(all_syms):\n",
    "        cue_dict[s] = input_vals[i]\n",
    "    \n",
    "    # load data for primitive training\n",
    "    trainseqs = change_dict(trainseqs_old, cue_dict)\n",
    "    train_inputs = convert_seq2inputs(trainseqs, num_classes=num_classes, seq_len=5)\n",
    "    trainset = DataLoader(train_inputs, batch_size=batchsize, shuffle=True)\n",
    "    testseqs = change_dict(testseqs_old, cue_dict)\n",
    "    test_inputs = convert_seq2inputs(testseqs, num_classes=num_classes, seq_len=5)\n",
    "    testset = DataLoader(test_inputs, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    # run primitive training RNN\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, trainset, [trainset, testset], epochs)\n",
    "    \n",
    "    return {'cue_dict':cue_dict, 'loss':loss1, 'acc':acc1, 'mod': model, 'test': testset}\n",
    "\n",
    "def run_plot(trainseqs, testseqs):\n",
    "    t1 = time.time()\n",
    "    res  = Parallel(n_jobs = -1)(delayed(run_exp)(trainseqs, testseqs) for i in range(num_sims))\n",
    "    t2 = time.time()\n",
    "    print('time: ', (t2-t1)/60)\n",
    "\n",
    "    losses = np.hstack([r['loss'] for r in res])\n",
    "    acc_train = np.array([r['acc'][:,0] for r in res]).T\n",
    "    acc_test = np.array([r['acc'][:,1] for r in res]).T\n",
    "    mods = [r['mod'] for r in res]\n",
    "    tests = [r['test'] for r in res]\n",
    "\n",
    "    plotNNs([losses], [acc_train, acc_test], \\\n",
    "            ['train', 'test'], colors = ['green', 'orange'], title = '', shift = 0)\n",
    "    \n",
    "    plt.figure()\n",
    "    r2, dfs = predcorr(mods, tests)\n",
    "    \n",
    "    return res, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a45e2e",
   "metadata": {},
   "source": [
    "# Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f1955c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 0, 'A': 17, 'B': 4, 'C': 14, 'D': 7, 'E': 5}\n"
     ]
    }
   ],
   "source": [
    "total_syms = ['A','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "all_syms = total_syms[:num_inputs]\n",
    "all_input_vals = list(np.arange(2,18))\n",
    "input_vals = random.sample(all_input_vals,num_inputs)\n",
    "#Â randomly select values for each input\n",
    "cue_dict = {'X':0}\n",
    "for i, s in enumerate(all_syms):\n",
    "    cue_dict[s] = input_vals[i]\n",
    "    \n",
    "print(cue_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcb37b",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "1. \n",
    "2. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fea116",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_inputcue = {'X': 0,\n",
    "                    'Y': 1,\n",
    "                    'A': 2, \n",
    "                    'B': 3,\n",
    "                    'C': 4,\n",
    "                    'D': 5,\n",
    "                    'E': 6, \n",
    "                    'F': 7,\n",
    "                    'G': 8,\n",
    "                    'H': 9,\n",
    "                    'I': 10, \n",
    "                    'J': 11,\n",
    "                    'K': 12,\n",
    "                    'L': 13,\n",
    "                    'M': 14, \n",
    "                    'N': 15,\n",
    "                    'O': 16,\n",
    "                    'P': 17\n",
    "                    }\n",
    "\n",
    "convert_operation = {'+': 18,\n",
    "                     '*': 19,\n",
    "                     '-': 20,\n",
    "                     '=': 21}\n",
    "\n",
    "onehot_dict = {0:'X',\n",
    "                1:'Y',\n",
    "                2:'A', \n",
    "                3:'B',\n",
    "                4:'C',\n",
    "                5:'D',\n",
    "                6:'E', \n",
    "                7:'F',\n",
    "                8:'G',\n",
    "                9:'H',\n",
    "                10:'I', \n",
    "                11:'J',\n",
    "                12:'K',\n",
    "                13:'L',\n",
    "                14:'M', \n",
    "                15:'N',\n",
    "                16:'O',\n",
    "                17:'P',\n",
    "                18:'+',\n",
    "                19:'*',\n",
    "                20:'-',\n",
    "                21:'=',\n",
    "                    }\n",
    "          \n",
    "\n",
    "def operate_op(currval, step_tuple, cue_dict):\n",
    "    \"\"\" Function applies operations to input value\n",
    "    \"\"\"\n",
    "    nextval = cue_dict[step_tuple[1]]\n",
    "    if step_tuple[0] == '+': # add\n",
    "        currval = currval + nextval\n",
    "    elif step_tuple[0] == '*': # multiply\n",
    "        currval = currval * nextval\n",
    "    elif step_tuple[0] == '-': # subtract\n",
    "        currval = currval - nextval\n",
    "    \n",
    "    return currval\n",
    "\n",
    "def calculate_output(step_tuple_full, cue_dict):\n",
    "    \"\"\" Function applies operations to input value\"\"\"\n",
    "    step_tuple = step_tuple_full[:-1]\n",
    "    curr_val = cue_dict[step_tuple[0]]\n",
    "    for i in range(1,len(step_tuple)):\n",
    "        curr_val = operate_op(curr_val, step_tuple[i], cue_dict)\n",
    "    return curr_val\n",
    "\n",
    "def unique(list1):\n",
    "    unique_list = []\n",
    "    for x in list1:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list\n",
    "\n",
    "\n",
    "##################################################\n",
    "## Transform data to rnn data\n",
    "##################################################\n",
    "\n",
    "\n",
    "class SequenceData(Dataset):\n",
    "    def __init__(self, data, labels, seq_len, stages, cont_out):\n",
    "\n",
    "        self.data = convert_seq2onehot(data, stages)\n",
    "        self.seq_len = seq_len\n",
    "        if cont_out:\n",
    "            self.labels = labels\n",
    "        else:\n",
    "            self.labels = convert_outs2labels(labels)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sequence = self.data[index].astype(np.float32)\n",
    "        out_state = np.array(self.labels[index]).astype(np.float32)\n",
    "        return sequence, out_state\n",
    "    \n",
    "    \n",
    "def convert_seq2inputs(sequences, seq_len=5, stages = False, cont_out = True, num_classes=22):\n",
    "    '''\n",
    "    Function converts sequences as they are generated by generate_experiment_lists.py\n",
    "    into input to be fed into RNN (one-hote encoded)\n",
    "    Parameters:\n",
    "        sequences: list of trials with format : [initial_value, (operation, input_cue) ... , output_value]\n",
    "        num_classes: total number of features for onehot encoding\n",
    "        seq_len: number of time steps per sequence\n",
    "        stages: if False each unit is a time step, if True each tuple is a time step\n",
    "        cont_out: if True the output is continuous, if False output is categorical\n",
    "    ''' \n",
    "    seq = [sublist[:-1] for sublist in sequences]\n",
    "    out = [sublist[-1] for sublist in sequences]\n",
    "    \n",
    "    seqdata = SequenceData(seq, out, seq_len, stages, cont_out)\n",
    "\n",
    "    return seqdata\n",
    "\n",
    "\n",
    "def convert_seq2onehot(seq, stages, num_classes=22):\n",
    "    \"\"\" Function ...\n",
    "    Args:\n",
    "        ...\n",
    "    Returns:\n",
    "        ...\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for trial in seq:\n",
    "        trial_data = []\n",
    "        for i,t in enumerate(trial):\n",
    "            if i==0:\n",
    "                init = torch.tensor(convert_inputcue[t])\n",
    "                init = torch.nn.functional.one_hot(init, num_classes=num_classes)\n",
    "                trial_data.append(init)\n",
    "                continue\n",
    "            elif t == \"=\":\n",
    "                equals_sign = torch.tensor(convert_operation[t])\n",
    "                equals_sign = torch.nn.functional.one_hot(equals_sign, num_classes=num_classes)\n",
    "                trial_data.append(equals_sign)\n",
    "                continue\n",
    "            else:\n",
    "                op = torch.tensor(convert_operation[t[0]])\n",
    "                op = torch.nn.functional.one_hot(op, num_classes=num_classes)\n",
    "                inputcue = torch.tensor(convert_inputcue[t[1]])\n",
    "                inputcue = torch.nn.functional.one_hot(inputcue, num_classes=num_classes)\n",
    "                trial_data.append(op)\n",
    "                trial_data.append(inputcue)\n",
    "        data.append(torch.stack(trial_data).numpy())\n",
    "\n",
    "#     data = torch.stack(data,dim=-1) #combine into tensor of shape n_trials X n_time_steps X inputvector_size\n",
    "#     data = data.numpy()\n",
    "\n",
    "    return data\n",
    "\n",
    "def onehot2seq(seqs):\n",
    "    curr_trial = []\n",
    "    for seq in seqs:\n",
    "        for step in seq:\n",
    "            curr_trial.append(onehot_dict[np.argmax(step).item()])\n",
    "    return curr_trial\n",
    "\n",
    "\n",
    "def convert_outs2labels(outputs, num_outs=1000):\n",
    "    \"\"\" Function ...\n",
    "    Args:\n",
    "        ...\n",
    "    Returns:\n",
    "        ...\n",
    "    \"\"\"\n",
    "    all_outs = []\n",
    "    for out in outputs:\n",
    "        out = torch.tensor(out)\n",
    "        onehot_out = torch.nn.functional.one_hot(out, num_classes = num_outs)\n",
    "        all_outs.append(onehot_out)\n",
    "    return all_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30271da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d62f2e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainseqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6y/z1h4y1t94jvb186qws4p3qq40000gn/T/ipykernel_45400/2051540222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainseqs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainseqs' is not defined"
     ]
    }
   ],
   "source": [
    "trainseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c73b1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = ['-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "096bb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "p = generate_primitives(all_syms)\n",
    "trainseqs = generate_trials(ops, all_syms[1:], all_syms[1:])+ p\n",
    "testseqs = generate_trials(ops, all_syms, all_syms)\n",
    "testseqs = [t for t in testseqs if t not in trainseqs] \n",
    "trainseqs.append(testseqs.pop(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85b9af27",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6y/z1h4y1t94jvb186qws4p3qq40000gn/T/ipykernel_45400/3469354767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/6y/z1h4y1t94jvb186qws4p3qq40000gn/T/ipykernel_45400/2360629429.py\u001b[0m in \u001b[0;36mrun_plot\u001b[0;34m(trainseqs, testseqs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mres\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainseqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestseqs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_sims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "res1, r2_1 = run_plot(trainseqs, testseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9b3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f6c90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = generate_primitives(all_syms)\n",
    "trainseqs = generate_other_pairs(ops[0], all_syms[1:]) + p\n",
    "testseqs = generate_trials(ops, all_syms, all_syms)\n",
    "testseqs = [t for t in testseqs if t not in trainseqs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9876906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A', ('-', 'A'), '=', 0],\n",
       " ['A', ('-', 'B'), '=', 13],\n",
       " ['A', ('-', 'C'), '=', 3],\n",
       " ['A', ('-', 'D'), '=', 10],\n",
       " ['A', ('-', 'E'), '=', 12],\n",
       " ['B', ('-', 'A'), '=', -13],\n",
       " ['B', ('-', 'B'), '=', 0],\n",
       " ['B', ('-', 'D'), '=', -3],\n",
       " ['B', ('-', 'E'), '=', -1],\n",
       " ['C', ('-', 'A'), '=', -3],\n",
       " ['C', ('-', 'B'), '=', 10],\n",
       " ['C', ('-', 'C'), '=', 0],\n",
       " ['C', ('-', 'E'), '=', 9],\n",
       " ['D', ('-', 'A'), '=', -10],\n",
       " ['D', ('-', 'B'), '=', 3],\n",
       " ['D', ('-', 'C'), '=', -7],\n",
       " ['D', ('-', 'D'), '=', 0],\n",
       " ['E', ('-', 'A'), '=', -12],\n",
       " ['E', ('-', 'C'), '=', -9],\n",
       " ['E', ('-', 'D'), '=', -2],\n",
       " ['E', ('-', 'E'), '=', 0]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1, r2_1 = run_plot(trainseqs, testseqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a2e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
