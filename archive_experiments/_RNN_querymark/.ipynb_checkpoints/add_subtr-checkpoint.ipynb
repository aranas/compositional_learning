{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c1245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "\n",
    "# ======\n",
    "current = os.path.dirname(os.path.realpath('plotting.py'))\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "from functions.rnn_cryptic_equals import convert_seq2inputs, calculate_output, onehot2seq "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1c75e",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7103e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(OneStepRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers= num_layers,\n",
    "                        batch_first=True)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define the layers\n",
    "        self.input2hidden = nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        self.fc1tooutput = nn.Linear(self.hidden_size, output_size)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), dim=0) ## dim = 1??\n",
    "        self.hidden = nn.functional.relu(self.input2hidden(combined))\n",
    "        self.output = self.fc1tooutput(self.hidden)\n",
    "        #return self.output.view(-1,output_size), self.hidden\n",
    "        return self.output, self.hidden\n",
    "\n",
    "    def get_activations(self, x, hidden):\n",
    "        self.forward(x, hidden)  # update the activations with the particular input\n",
    "        return self.hidden, self.output #, self.fc1_activations\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.hidden_noise\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)[0]\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight, 0.1)\n",
    "\n",
    "\n",
    "def train(sequence, label ,model ,optimizer ,criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #Read each cue in and keep hidden state for next cue\n",
    "    hidden = model.initHidden()\n",
    "    batch_out = []\n",
    "    for batchseq in sequence:\n",
    "        for i in range(len(batchseq)):\n",
    "            output, hidden = model.forward(batchseq[i], hidden)\n",
    "        batch_out.append(output)\n",
    "        #Compare final output to target\n",
    "    batch_out = torch.cat(batch_out)\n",
    "    loss = criterion(batch_out,label)#.long())\n",
    "\n",
    "    #Back-propagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return batch_out, loss.item()\n",
    "\n",
    "def run_acc(model,optimizer,criterion, train_data, test_data, epochs, verbose = False):\n",
    "    \n",
    "    loss_history = np.empty((0,1))\n",
    "    all_accs = np.empty((0,len(test_data)))\n",
    "    for epoch in range(epochs):\n",
    "        lossTotal = 0\n",
    "        for i, (x,y) in enumerate(train_data):\n",
    "            output, loss = train(x,y,model,optimizer,criterion)\n",
    "            lossTotal += loss # add MSE -> sum of square errors \n",
    "        loss_history = np.vstack([loss_history, lossTotal])\n",
    "        acc = test_acc(model, test_data, hidden_size)\n",
    "        all_accs = np.vstack([all_accs,acc])\n",
    "\n",
    "    return loss_history, all_accs\n",
    "\n",
    "def test_acc(model, testdata, hidden_size, verbose = False):\n",
    "    model.eval()\n",
    "    accs = np.empty((1, 0))\n",
    "    for testset in testdata:\n",
    "        batch_correct = []\n",
    "        for x,y in testset:\n",
    "            correct = 0\n",
    "            for i in range(len(x)):\n",
    "                hidden = torch.zeros(1, hidden_size)[0]\n",
    "                for step in x[i]:\n",
    "                    hidden, y_hat = model.get_activations(step,hidden)\n",
    "                correct += sum(torch.round(y[i]) == torch.round(y_hat)).item()\n",
    "            batch_correct.append(correct/len(y))\n",
    "        acc = np.mean(batch_correct)\n",
    "        accs = np.append(accs, [acc])\n",
    "    if verbose:\n",
    "        print('test accuracy: %f ' % (acc))\n",
    "    return accs\n",
    "\n",
    "def test_preds(model, testdata, hidden_size, suffix = ''):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labs = []\n",
    "    trials = []\n",
    "    accs = []\n",
    "    for testset in testdata:\n",
    "        batch_correct = []\n",
    "        for x,y in testset:\n",
    "            for i in range(len(x)):\n",
    "                hidden = torch.zeros(1, hidden_size)[0]\n",
    "                for step in x[i]:\n",
    "                    hidden, y_hat = model.get_activations(step,hidden)\n",
    "                preds.append(y_hat.detach().item())\n",
    "                labs.append(y[i].detach().item())\n",
    "                correct = sum(torch.round(y[i]) == torch.round(y_hat)).item()\n",
    "                accs.append(correct)\n",
    "            trials.append(str(onehot2seq(x)))\n",
    "    df = pd.DataFrame({'trial':trials, 'label'+suffix:labs, 'pred'+suffix: preds, 'acc'+suffix: accs})\n",
    "    return df \n",
    "\n",
    "\n",
    "def shuffle_weights(model):\n",
    "    model2 = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    mod_dict = model.state_dict()\n",
    "    shuffled_dict = {layer: shuffle_tensor(val) for layer, val in mod_dict.items()}\n",
    "    model2.load_state_dict(shuffled_dict)\n",
    "    return model2\n",
    "\n",
    "def shuffle_tensor(t):\n",
    "    idx = torch.randperm(t.nelement())\n",
    "    t = t.view(-1)[idx].view(t.size())\n",
    "    return t\n",
    "\n",
    "def run_sim(train_trials, test_trials):\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, train_trials, test_trials, epochs)\n",
    "    return loss1, acc1, model\n",
    "\n",
    "def run_sims(i, train_trials, test_trials):\n",
    "    print('##########Â rep', i, '#########')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, train_trials[0], test_trials, epochs)\n",
    "    loss2, acc2 = run_acc(model,optimizer,criterion, train_trials[1], test_trials, epochs)\n",
    "    losses = np.vstack([loss1,loss2])\n",
    "    accs = np.vstack([acc1,acc2])\n",
    "    return losses, accs, model\n",
    "\n",
    "\n",
    "def change_dict(seqs, new_dict):\n",
    "    \"\"\" recalculates sequence output\"\"\"\n",
    "    inps = [s[:-1] for s in seqs]\n",
    "    for inp in inps:\n",
    "        inp.append(calculate_output(inp, new_dict))\n",
    "\n",
    "    return inps\n",
    "\n",
    "def predcorr(mods, tests, plot_corr = True):\n",
    "    dfs1 = []\n",
    "    for i in range(len(mods)):\n",
    "        df = test_preds(mods[i], [tests[i]], hidden_size)\n",
    "        dfs1.append(df)\n",
    "    all_dfs1 = pd.concat(dfs1) \n",
    "    preds, labs = all_dfs1['pred'], all_dfs1['label']\n",
    "    xy = np.arange(np.min(preds)-1, np.max(labs)+1, 0.1)\n",
    "    r2_val = r2_score(all_dfs1['pred'],all_dfs1['label'])\n",
    "    df_fin = all_dfs1.groupby(['trial']).mean().sort_values(by = 'acc' , ascending=False)\n",
    "    if plot_corr:\n",
    "        for d in dfs1:\n",
    "            plt.scatter(d['label'], d['pred'])\n",
    "        plt.plot(xy,xy)\n",
    "        plt.xlabel('Ground truth')\n",
    "        plt.ylabel('Model prediction')\n",
    "        plt.title('with primitive training, R^2 = ' + str(round(r2_val, 2)) )\n",
    "             \n",
    "    return r2_val, df_fin, all_dfs1  \n",
    "\n",
    "# -----------------\n",
    "# Generating trials\n",
    "# -----------------\n",
    "\n",
    "def generate_primitives(inputs):\n",
    "    seq = []\n",
    "    for inp in inputs:\n",
    "        trial = [inp, '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "\n",
    "def generate_trials(op, input_ids, init_values):\n",
    "    \n",
    "    ''' function for generating all permutations of 1 step trials '''\n",
    "    \n",
    "    seq = []\n",
    "    combi_inputcue = list(itertools.product(input_ids, repeat=1))\n",
    "    for init in init_values:\n",
    "        for cue in combi_inputcue:\n",
    "            seq.append([init,\n",
    "                        *zip(tuple(op), cue), '=']) #group per time point t\n",
    "    for s in seq:\n",
    "        s.append(calculate_output(s, cue_dict))\n",
    "    return seq\n",
    "\n",
    "def generate_self(op, inputs):\n",
    "    seq = []\n",
    "    for inp in inputs:\n",
    "        trial = generate_trials(op, inp, inp)\n",
    "        seq += trial\n",
    "    return seq\n",
    "\n",
    "def generate_other(op, inputs):\n",
    "    seq = []\n",
    "    inputs1 = inputs.copy()\n",
    "    inputs2 = inputs.copy()\n",
    "    inputs2.append(inputs2.pop(0))\n",
    "    for i in range(len(inputs1)):\n",
    "        trial = [inputs1[i], (op, inputs2[i]), '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "def generate_other(op, inputs):\n",
    "    seq = []\n",
    "    inputs1 = inputs.copy()\n",
    "    inputs2 = inputs.copy()\n",
    "    inputs2.append(inputs2.pop(0))\n",
    "    for i in range(len(inputs1)):\n",
    "        trial = [inputs1[i], (op, inputs2[i]), '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "def generate_other_reverse(op, inputs):\n",
    "    seq = []\n",
    "    inputs1 = inputs.copy()\n",
    "    inputs2 = inputs.copy()\n",
    "    inputs2.append(inputs2.pop(0))\n",
    "    inputs2.append(inputs2.pop(0))\n",
    "    for i in range(len(inputs1)):\n",
    "        trial = [inputs1[i], (op, inputs2[i]), '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "def generate_other_reverse_primitives(op, inputs):\n",
    "    seq = []\n",
    "    inputs1 = inputs.copy()\n",
    "    inputs2 = inputs.copy()\n",
    "    n = int(len(inputs1)/2)\n",
    "    for i in range(n):\n",
    "        trial = [inputs1[i], (op, inputs2[i+n]), '=']\n",
    "        trial.append(calculate_output(trial, cue_dict))\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "def generate_neg_primitives(inputs):\n",
    "    seq = []\n",
    "    for inp in inputs:\n",
    "        trial = [('-', inp), '=']\n",
    "        trial.append(-1*cue_dict[inp])\n",
    "        seq.append(trial)\n",
    "    return seq\n",
    "\n",
    "def generate_2step_trials(op, input_ids, init_values):\n",
    "    \n",
    "    ''' function for generating all permutations of 1 step trials '''\n",
    "    \n",
    "    seq = []\n",
    "    combi_inputcue = list(itertools.product(input_ids, repeat=2))\n",
    "    combi_operators = list(itertools.product(ops, repeat=2))\n",
    "    for init in init_values:\n",
    "        for cues in combi_inputcue:\n",
    "            for operators in combi_operators:\n",
    "                seq.append([init,\n",
    "                            *zip(tuple(operators), cues), '=']) #group per time point t\n",
    "    for s in seq:\n",
    "        s.append(calculate_output(s, cue_dict))\n",
    "    return seq\n",
    "\n",
    "# ----------\n",
    "# plotting\n",
    "# ----------\n",
    "\n",
    "def plotNNs(loss_arrays, acc_arrays, labels, colors, title, shift = 0):\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    loss_cols = ['blue', 'darkblue']\n",
    "    loss_labs = ['loss_with_primitive', 'loss_without_primitive']\n",
    "    for i, arr in enumerate(loss_arrays):\n",
    "        x = np.arange(0,arr.shape[0],1) + shift\n",
    "        mn = arr.mean(axis=1)\n",
    "        errs = arr.std(axis=1)\n",
    "        \n",
    "        axs[0].plot(x, mn, label = loss_labs[i], color = loss_cols[i])\n",
    "        axs[0].fill_between(x, mn - errs, mn + errs, alpha = 0.3, facecolor = loss_cols[i])\n",
    "    \n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].set_ylabel('loss')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    for i, arr in enumerate(acc_arrays):\n",
    "        x = np.arange(0,arr.shape[0],1) + shift\n",
    "        mn = arr.mean(axis=1)\n",
    "        errs = arr.std(axis=1)\n",
    "        \n",
    "        axs[1].plot(x, mn, label = labels[i], color = colors[i])\n",
    "        axs[1].fill_between(x, mn - errs, mn + errs, alpha = 0.3, facecolor = colors[i])\n",
    "    \n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].set_ylabel('accuracy')\n",
    "    axs[1].legend()\n",
    "\n",
    "    fig.suptitle(title, fontsize=10)\n",
    "    \n",
    "def plot_accs(acc_arrays, labels, colors, title, shift = 0):\n",
    "\n",
    "    for i, arr in enumerate(acc_arrays):\n",
    "        x = np.arange(0,arr.shape[0],1) + shift\n",
    "        mn = arr.mean(axis=1)\n",
    "        errs = arr.std(axis=1)\n",
    "        \n",
    "        plt.plot(x, mn, label = labels[i], color = colors[i])\n",
    "        plt.fill_between(x, mn - errs, mn + errs, alpha = 0.3, facecolor = colors[i])\n",
    "    \n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.title(title, fontsize=10)\n",
    "\n",
    "    \n",
    "def heatmap_acc(num_inputs, df, ax):\n",
    "    \n",
    "    total_syms = ['A','B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P']\n",
    "    map_syms = total_syms[:num_inputs]\n",
    "    data_accs = np.empty((num_inputs, num_inputs))\n",
    "    data_accs[:] = np.NaN\n",
    "    for r, trial in enumerate(df.index):\n",
    "        i = map_syms.index(eval(trial)[0])\n",
    "        j = map_syms.index(eval(trial)[2])\n",
    "        acc = round(df.iloc[r]['acc'], 2)\n",
    "        data_accs[i,j] = acc\n",
    "    \n",
    "    # Show all ticks and label them with the respective list entries\n",
    "    ax.set_xticks(np.arange(num_inputs), labels=map_syms)\n",
    "    ax.set_yticks(np.arange(num_inputs), labels=map_syms)\n",
    "\n",
    "    #cmap = mpl.colors.ListedColormap(['yellow', 'orange', 'darkorange','red'])\n",
    "    from matplotlib import cm\n",
    "    new_reds = cm.get_cmap('Reds', 10)\n",
    "    cmap=new_reds\n",
    "    bounds = list(np.arange(0,1.1,0.1))\n",
    "    norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    im = ax.imshow(data_accs, cmap=cmap, norm=norm)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(num_inputs):\n",
    "        for j in range(num_inputs):\n",
    "            if np.isnan(data_accs[i, j]):\n",
    "                pass\n",
    "            else:\n",
    "                text = ax.text(j,i, data_accs[i, j],\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943fd65d",
   "metadata": {},
   "source": [
    "# RNN setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e5b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## running function\n",
    "\n",
    "def run_exp(trainseqs_old, testseqs_old1, testseqs_old2, cue_dict):\n",
    " \n",
    "    all_syms = list(cue_dict.keys())\n",
    "    all_input_vals = list(np.arange(2,18))\n",
    "    input_vals = random.sample(all_input_vals,num_inputs)\n",
    "    #Â randomly select values for each input\n",
    "    for i, s in enumerate(all_syms):\n",
    "        cue_dict[s] = input_vals[i]\n",
    "    \n",
    "    # load data for primitive training\n",
    "    trainseqs = change_dict(trainseqs_old, cue_dict)\n",
    "    train_inputs = convert_seq2inputs(trainseqs, num_classes=num_classes, seq_len=5)\n",
    "    trainset = DataLoader(train_inputs, batch_size=batchsize, shuffle=True)\n",
    "    testseqs = change_dict(testseqs_old1, cue_dict)\n",
    "    test_inputs = convert_seq2inputs(testseqs, num_classes=num_classes, seq_len=5)\n",
    "    testset1 = DataLoader(test_inputs, batch_size=batchsize, shuffle=True)\n",
    "    testseqs = change_dict(testseqs_old2, cue_dict)\n",
    "    test_inputs = convert_seq2inputs(testseqs, num_classes=num_classes, seq_len=5)\n",
    "    testset2 = DataLoader(test_inputs, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    # run primitive training RNN\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model,optimizer,criterion, trainset, [trainset, testset1, testset2], epochs)\n",
    "    \n",
    "    return {'cue_dict':cue_dict, 'loss':loss1, 'acc':acc1, 'mod': model, 'test1': testset1, 'test2': testset2}\n",
    "\n",
    "def run_plot(trainseqs, testseqs1, testseqs2,cue_dict):\n",
    "    t1 = time.time()\n",
    "    res  = Parallel(n_jobs = -1)(delayed(run_exp)(trainseqs, testseqs1,testseqs2, cue_dict) for i in range(num_sims))\n",
    "    t2 = time.time()\n",
    "    print('run time: ', (t2-t1)/60)\n",
    "\n",
    "    losses = np.hstack([r['loss'] for r in res])\n",
    "    acc_train = np.array([r['acc'][:,0] for r in res]).T\n",
    "    acc_test1 = np.array([r['acc'][:,1] for r in res]).T\n",
    "    acc_test2 = np.array([r['acc'][:,2] for r in res]).T\n",
    "    mods = [r['mod'] for r in res]\n",
    "    tests1 = [r['test1'] for r in res]\n",
    "    tests2 = [r['test2'] for r in res]\n",
    "\n",
    "    plotNNs([losses], [acc_train, acc_test1, acc_test2], \\\n",
    "            ['train', 'test1', 'test2'], colors = ['green', 'orange', 'red'], title = '', shift = 0)\n",
    "    \n",
    "    final_accs = pd.DataFrame({'train_acc': acc_train[-1,:],'test_acc1': acc_test1[-1,:], 'test_acc2': acc_test2[-1,:]}) \n",
    "    \n",
    "    fully_trained = final_accs[final_accs['train_acc'] == 1]\n",
    "    fully_trained_idx = fully_trained.index\n",
    "    fully_trained_test_acc1 = fully_trained['test_acc1'].mean() \n",
    "    fully_trained_test_acc_std1 = fully_trained['test_acc1'].std()\n",
    "    fully_trained_test_acc2 = fully_trained['test_acc2'].mean() \n",
    "    fully_trained_test_acc_std2 = fully_trained['test_acc2'].std()\n",
    "    \n",
    "    ft_mods = [mods[i] for i in fully_trained_idx]\n",
    "    print(len(ft_mods))\n",
    "    ft_tests1 = [tests1[i] for i in fully_trained_idx]\n",
    "    ft_tests2 = [tests2[i] for i in fully_trained_idx]\n",
    "    if len(ft_mods) > 0:\n",
    "        plt.figure()\n",
    "        r2_1, dfs_1, alldfs_1 = predcorr(ft_mods, ft_tests1, plot_corr = True)\n",
    "        plt.figure()\n",
    "        r2_2, dfs_2, alldfs_2 = predcorr(ft_mods, ft_tests2, plot_corr = True)\n",
    "    else:\n",
    "        r2, dfs, alldfs = 0, 0, 0\n",
    "\n",
    "    t3 = time.time()\n",
    "    print('total time: ', (t3-t1)/60)\n",
    "    print(fully_trained.shape[0], '/', final_accs.shape[0], ' rnns trained to 100% accuracy.\\\n",
    "        \\n Mean test acc = ', round(fully_trained_test_acc1, 2), '\\n2 step acc: ',round(fully_trained_test_acc2, 2), '\\n-----------\\n')\n",
    "\n",
    "    return {'res':res,'mean_acc_final1': fully_trained_test_acc1,'mean_acc_final2': fully_trained_test_acc2, \\\n",
    "            'df_trials1': dfs_1, 'df_trials2': dfs_2,'acc_std1': fully_trained_test_acc_std1,\\\n",
    "           'acc_std2': fully_trained_test_acc_std2, 'ft_mods': ft_mods, 'ft_tests1': ft_tests1, 'ft_tests2': ft_tests2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef2d03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "num_classes = 22\n",
    "batchsize=1\n",
    "\n",
    "# RNN specs\n",
    "input_size = num_classes\n",
    "output_size = 1\n",
    "num_layers = 1\n",
    "hidden_size = 20\n",
    "learningRate = 0.005\n",
    "\n",
    "## running params\n",
    "epochs = 1000\n",
    "num_sims = 104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bcb37b",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Final accuracy against ratio of other/self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f92b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87b23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b833829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
