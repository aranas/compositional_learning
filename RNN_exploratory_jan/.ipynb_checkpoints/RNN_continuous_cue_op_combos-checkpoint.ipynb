{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38a1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad0ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current = os.path.dirname(os.path.realpath('plotting.py'))\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "952157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functions.plotting as NNplt\n",
    "from functions.rnn_cryptic import generate_sequences, convert_seq2inputs, pad_select, pad_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3032c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):\n",
    "    unique_list = []\n",
    "    for x in list1:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    return unique_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef1333",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d099c029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super(OneStepRNN, self).__init__()\n",
    "        # Define parameters\n",
    "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers= num_layers,\n",
    "                        batch_first=True)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        # Define the layers\n",
    "        self.input2hidden = nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        self.fc1tooutput = nn.Linear(self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), dim=0) ## dim = 1??\n",
    "        self.hidden = nn.functional.relu(self.input2hidden(combined))\n",
    "        self.output = self.fc1tooutput(self.hidden)\n",
    "        #return self.output.view(-1,output_size), self.hidden\n",
    "        return self.output, self.hidden\n",
    "\n",
    "    def get_activations(self, x, hidden):\n",
    "        self.forward(x, hidden)  # update the activations with the particular input\n",
    "        return self.hidden, self.output #, self.fc1_activations\n",
    "\n",
    "    def get_noise(self):\n",
    "        return self.hidden_noise\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)[0]\n",
    "\n",
    "\n",
    "def train(sequence,label,model,optimizer,criterion):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    #Read each cue in and keep hidden state for next cue\n",
    "    hidden = model.initHidden()\n",
    "    for i in range(len(sequence[0])):\n",
    "        output, hidden = model.forward(sequence[0][i], hidden)\n",
    "    #Compare final output to target\n",
    "    loss = criterion(output,label)#.long())\n",
    "    #Back-propagate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "def run(model, train_data, epochs):\n",
    "\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for epoch in range(epochs):\n",
    "        lossTotal = 0\n",
    "        for x,y in train_data:\n",
    "            output, loss = train(x,y,model,optimizer,criterion)\n",
    "            lossTotal += loss # add MSE -> sum of square errors \n",
    "        loss_history.append(lossTotal)\n",
    "\n",
    "    print(f'loss: {round(lossTotal,1)} ')\n",
    "    return loss_history\n",
    "\n",
    "def run_acc(model, train_data, test_data, epochs, verbose = False):\n",
    "\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    for epoch in range(epochs):\n",
    "        lossTotal = 0\n",
    "        for i, (x,y) in enumerate(train_data):\n",
    "            output, loss = train(x,y,model,optimizer,criterion)\n",
    "            lossTotal += loss # add MSE -> sum of square errors \n",
    "            if (epoch%50 == 2):\n",
    "                if i%50 == 0:\n",
    "                    if verbose:\n",
    "                        print('\\n################\\nepoch: ',epoch, '\\n################\\n')\n",
    "                        print('output =. ' , output.detach().numpy())\n",
    "                        print('label =. ' , y.detach().numpy())\n",
    "                        test_acc(model, test_data, hidden_size, verbose = True)\n",
    "        loss_history.append(lossTotal)\n",
    "        acc = test_acc(model, test_data, hidden_size)\n",
    "        acc_history.append(acc)\n",
    "\n",
    "    print(f'loss: {round(lossTotal,1)} ')\n",
    "    print(f'accuracy: {round(acc,2)} ')\n",
    "    return loss_history, acc_history\n",
    "\n",
    "\n",
    "\n",
    "def run_cvrmse(model, train_data, epochs):\n",
    "\n",
    "    model.train()\n",
    "    rmse_history = []\n",
    "    for epoch in range(epochs):\n",
    "        lossTotal = 0\n",
    "        ylog = 0\n",
    "        for x,y in train_data:\n",
    "            ylog += y.mean().item()\n",
    "            output, loss = train(x,y,model,optimizer,criterion)\n",
    "            lossTotal += loss # add MSE -> sum of square errors \n",
    "        rmse = (lossTotal/len(train_data))**0.5 # root mean square error\n",
    "        cvrmse = rmse/(ylog/len(train_data))\n",
    "        rmse_history.append(cvrmse)\n",
    "\n",
    "    print(f'cv rmse: {round(cvrmse,2)} ')\n",
    "    return rmse_history\n",
    "\n",
    "\n",
    "def test_model(model, testdata, hidden_size, verbose=False):\n",
    "    model.eval()\n",
    "    SSE = 0\n",
    "    yvals = []\n",
    "    for x,y in testdata:\n",
    "        yvals.append(y.item())\n",
    "        hidden = torch.zeros(1, hidden_size)[0]\n",
    "        for step in x[0]:\n",
    "            hidden, y_hat = model.get_activations(step,hidden)\n",
    "        SE = (y.item() - y_hat.detach().item())**2 # squared error\n",
    "        SSE += SE # sum of square error\n",
    "    RMSE = (SSE/len(yvals))**0.5\n",
    "    CV_RMSE = 100*RMSE/np.mean(yvals)\n",
    "    \n",
    "    return CV_RMSE\n",
    "\n",
    "def test_acc(model, testdata, hidden_size, verbose = False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for x,y in testdata:\n",
    "        hidden = torch.zeros(1, hidden_size)[0]\n",
    "        for step in x[0]:\n",
    "            hidden, y_hat = model.get_activations(step,hidden)\n",
    "        correct += sum(torch.round(y) == torch.round(y_hat)).item()/len(y)\n",
    "    acc = correct/len(testdata)\n",
    "    \n",
    "    if verbose:\n",
    "        print('test accuracy: %f ' % (acc))\n",
    "\n",
    "    return acc\n",
    "\n",
    "def shuffle_weights(model):\n",
    "    model2 = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    mod_dict = model.state_dict()\n",
    "    shuffled_dict = {layer: shuffle_tensor(val) for layer, val in mod_dict.items()}\n",
    "    model2.load_state_dict(shuffled_dict)\n",
    "    return model2\n",
    "\n",
    "def shuffle_statedict(mod_dict):\n",
    "    return {layer: shuffle_tensor(val) for layer, val in mod_dict.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6733e3",
   "metadata": {},
   "source": [
    "# Experiments:\n",
    "1 step pretraining (compare 1 step pretraining to 2)\n",
    "\n",
    "primitive op (work out how operations work)\n",
    "\n",
    "primitive cue (work out number corresponding to cue id)\n",
    "\n",
    "...stages vs sentence trial structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72becd36",
   "metadata": {},
   "source": [
    "# 1 step pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0caee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "32\n",
      "128\n",
      "train 1:  96\n",
      "test 1:  32\n",
      "train 2:  96\n",
      "test 2:  32\n",
      "train 3:  96\n",
      "test 3:  32\n",
      "results/step_pretraining/batchsize_1/\n"
     ]
    }
   ],
   "source": [
    "# Generate data\n",
    "\n",
    "operators = ['+', '*']\n",
    "input_ids = ['A', 'B'] \n",
    "init_values = [5,7]\n",
    "\n",
    "# generate trial data\n",
    "batchsize = 1\n",
    "def generate_step_trials(len_seq, scale):\n",
    "    seqs = generate_sequences(operators, input_ids, len_seq, init_values = init_values, rand = False)\n",
    "    print(len(seqs))\n",
    "    trainset, testset = train_test_split(seqs, test_size=1/4)\n",
    "    train_inputs = convert_seq2inputs(trainset*scale, num_classes=13, seq_len=5)\n",
    "    test_inputs = convert_seq2inputs(testset*scale, num_classes=13, seq_len=5)\n",
    "    train_data = DataLoader(train_inputs, batch_size=batchsize, shuffle=True)\n",
    "    test_data = DataLoader(test_inputs, batch_size=batchsize, shuffle=True)\n",
    "    return train_data, test_data\n",
    "\n",
    "    \n",
    "# 1 step trials\n",
    "train_data1, test_data1 = generate_step_trials(1, 16)\n",
    "\n",
    "# 2 step trials\n",
    "train_data2, test_data2 = generate_step_trials(2, 4)\n",
    "\n",
    "# 3 step trials\n",
    "train_data3, test_data3 = generate_step_trials(3, 1)\n",
    "\n",
    "print('train 1: ', len(train_data1))\n",
    "print('test 1: ', len(test_data1))\n",
    "print('train 2: ', len(train_data2))\n",
    "print('test 2: ', len(test_data2))\n",
    "print('train 3: ', len(train_data3))\n",
    "print('test 3: ', len(test_data3))\n",
    "\n",
    "save_dir = 'results/step_pretraining/batchsize_' + str(batchsize) + '/'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a67e04",
   "metadata": {},
   "source": [
    "# Step pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b04691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "num_classes = 13\n",
    "input_size = num_classes\n",
    "output_size = batchsize\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "learningRate = 0.0005\n",
    "epochs = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "da4d999d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### rep 0  ###\n",
      "loss: 148.1 \n",
      "accuracy: 0.0 \n",
      "loss: 682.2 \n",
      "accuracy: 0.0 \n",
      "loss: 655.3 \n",
      "accuracy: 0.0 \n",
      "### rep 1  ###\n",
      "loss: 146.8 \n",
      "accuracy: 0.0 \n",
      "loss: 540.8 \n",
      "accuracy: 0.0 \n",
      "loss: 635.4 \n",
      "accuracy: 0.0 \n",
      "### rep 2  ###\n",
      "loss: 59.6 \n",
      "accuracy: 0.5 \n",
      "loss: 616.9 \n",
      "accuracy: 0.0 \n",
      "loss: 609.0 \n",
      "accuracy: 0.12 \n",
      "### rep 3  ###\n",
      "loss: 79.8 \n",
      "accuracy: 0.0 \n",
      "loss: 579.1 \n",
      "accuracy: 0.0 \n",
      "loss: 835.4 \n",
      "accuracy: 0.0 \n",
      "### rep 4  ###\n",
      "loss: 150.7 \n",
      "accuracy: 0.0 \n",
      "loss: 478.2 \n",
      "accuracy: 0.12 \n",
      "loss: 942.1 \n",
      "accuracy: 0.12 \n",
      "### rep 5  ###\n",
      "loss: 82.4 \n",
      "accuracy: 0.0 \n",
      "loss: 568.1 \n",
      "accuracy: 0.0 \n",
      "loss: 674.0 \n",
      "accuracy: 0.12 \n",
      "### rep 6  ###\n",
      "loss: 156.2 \n",
      "accuracy: 0.0 \n",
      "loss: 584.0 \n",
      "accuracy: 0.0 \n",
      "loss: 869.7 \n",
      "accuracy: 0.12 \n",
      "### rep 7  ###\n",
      "loss: 163.1 \n",
      "accuracy: 0.0 \n",
      "loss: 825.2 \n",
      "accuracy: 0.12 \n",
      "loss: 713.5 \n",
      "accuracy: 0.0 \n",
      "### rep 8  ###\n",
      "loss: 151.8 \n",
      "accuracy: 0.0 \n",
      "loss: 595.4 \n",
      "accuracy: 0.0 \n",
      "loss: 778.4 \n",
      "accuracy: 0.12 \n",
      "### rep 9  ###\n",
      "loss: 154.1 \n",
      "accuracy: 0.0 \n",
      "loss: 572.1 \n",
      "accuracy: 0.0 \n",
      "loss: 600.3 \n",
      "accuracy: 0.12 \n",
      "### rep 10  ###\n",
      "loss: 145.8 \n",
      "accuracy: 0.0 \n",
      "loss: 798.2 \n",
      "accuracy: 0.12 \n",
      "loss: 693.5 \n",
      "accuracy: 0.0 \n",
      "### rep 11  ###\n",
      "loss: 97.7 \n",
      "accuracy: 0.0 \n",
      "loss: 596.1 \n",
      "accuracy: 0.0 \n",
      "loss: 784.2 \n",
      "accuracy: 0.0 \n",
      "### rep 12  ###\n",
      "loss: 141.6 \n",
      "accuracy: 0.0 \n",
      "loss: 521.1 \n",
      "accuracy: 0.0 \n",
      "loss: 650.3 \n",
      "accuracy: 0.12 \n",
      "### rep 13  ###\n",
      "loss: 76.5 \n",
      "accuracy: 0.0 \n",
      "loss: 349.4 \n",
      "accuracy: 0.0 \n",
      "loss: 664.0 \n",
      "accuracy: 0.12 \n",
      "### rep 14  ###\n",
      "loss: 115.4 \n",
      "accuracy: 0.0 \n",
      "loss: 576.8 \n",
      "accuracy: 0.0 \n",
      "loss: 1100.9 \n",
      "accuracy: 0.12 \n",
      "### rep 15  ###\n",
      "loss: 127.9 \n",
      "accuracy: 0.0 \n",
      "loss: 760.1 \n",
      "accuracy: 0.0 \n",
      "loss: 1410.7 \n",
      "accuracy: 0.12 \n",
      "### rep 16  ###\n",
      "loss: 137.8 \n",
      "accuracy: 0.0 \n",
      "loss: 507.4 \n",
      "accuracy: 0.12 \n",
      "loss: 729.4 \n",
      "accuracy: 0.0 \n",
      "### rep 17  ###\n",
      "loss: 88.9 \n",
      "accuracy: 0.0 \n",
      "loss: 354.4 \n",
      "accuracy: 0.0 \n",
      "loss: 659.8 \n",
      "accuracy: 0.0 \n",
      "### rep 18  ###\n",
      "loss: 122.3 \n",
      "accuracy: 0.0 \n",
      "loss: 643.9 \n",
      "accuracy: 0.12 \n",
      "loss: 1051.6 \n",
      "accuracy: 0.0 \n",
      "### rep 19  ###\n",
      "loss: 146.1 \n",
      "accuracy: 0.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-b5ee95e0ef0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mlosses12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0maccs12\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0macc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-d5e74b3151ed>\u001b[0m in \u001b[0;36mrun_acc\u001b[0;34m(model, train_data, test_data, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mlossTotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;31m# add MSE -> sum of square errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlossTotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0macc_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-d5e74b3151ed>\u001b[0m in \u001b[0;36mtest_acc\u001b[0;34m(model, testdata, hidden_size, verbose)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-d5e74b3151ed>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update the activations with the particular input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;31m#, self.fc1_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-179-d5e74b3151ed>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## dim = 1??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput2hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1tooutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#return self.output.view(-1,output_size), self.hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 2 step task\n",
    "\n",
    "num_sims = 40\n",
    "epochs = 50\n",
    "\n",
    "losses12 = []\n",
    "losses2 = []\n",
    "accs12 = []\n",
    "accs2 = []\n",
    "\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model, train_data1, test_data1, epochs)\n",
    "    \n",
    "    model2 = shuffle_weights(model)\n",
    "    \n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    losses12.append(loss1+loss2)\n",
    "    accs12.append(acc1+acc2)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model2.parameters(), lr=learningRate)\n",
    "    loss3, acc3 = run_acc(model2, train_data2, test_data2, epochs)\n",
    "    losses2.append(loss3)\n",
    "    accs2.append(acc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "702397cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_12 = np.array(losses12).T\n",
    "losses_2 = np.array(losses2).T\n",
    "\n",
    "NNplt.plotNN_losses([losses_12],\\\n",
    "                    labels = ['1 2 step pretrained'],\\\n",
    "                    colors = ['orange'])\n",
    "\n",
    "NNplt.plotNN_shifted([losses_2],\\\n",
    "                     labels = ['2 step pretrained'],\\\n",
    "                     colors = ['green'],\\\n",
    "                     shift = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ea7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119cfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "26294b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### rep 0  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.0 \n",
      "loss: 25.3 \n",
      "accuracy: 0.62 \n",
      "### rep 1  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 29.3 \n",
      "accuracy: 0.12 \n",
      "### rep 2  ###\n",
      "loss: 89.6 \n",
      "accuracy: 0.0 \n",
      "loss: 26.5 \n",
      "accuracy: 0.25 \n",
      "### rep 3  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.9 \n",
      "accuracy: 0.5 \n",
      "### rep 4  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.6 \n",
      "accuracy: 0.12 \n",
      "### rep 5  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.1 \n",
      "accuracy: 0.5 \n",
      "### rep 6  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.6 \n",
      "accuracy: 0.5 \n",
      "### rep 7  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 5.0 \n",
      "accuracy: 0.62 \n",
      "### rep 8  ###\n",
      "loss: 92.3 \n",
      "accuracy: 0.0 \n",
      "loss: 39.9 \n",
      "accuracy: 0.12 \n",
      "### rep 9  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 28.0 \n",
      "accuracy: 0.25 \n",
      "### rep 10  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 23.6 \n",
      "accuracy: 0.5 \n",
      "### rep 11  ###\n",
      "loss: 92.8 \n",
      "accuracy: 0.5 \n",
      "loss: 5.3 \n",
      "accuracy: 0.25 \n",
      "### rep 12  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.1 \n",
      "accuracy: 0.62 \n",
      "### rep 13  ###\n",
      "loss: 23.9 \n",
      "accuracy: 0.0 \n",
      "loss: 34.1 \n",
      "accuracy: 0.25 \n",
      "### rep 14  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.4 \n",
      "accuracy: 0.38 \n",
      "### rep 15  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 208.3 \n",
      "accuracy: 0.0 \n",
      "### rep 16  ###\n",
      "loss: 90.6 \n",
      "accuracy: 0.0 \n",
      "loss: 100.1 \n",
      "accuracy: 0.0 \n",
      "### rep 17  ###\n",
      "loss: 92.4 \n",
      "accuracy: 0.0 \n",
      "loss: 132.8 \n",
      "accuracy: 0.25 \n",
      "### rep 18  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 10.0 \n",
      "accuracy: 0.12 \n",
      "### rep 19  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.0 \n",
      "accuracy: 0.5 \n",
      "### rep 20  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.1 \n",
      "accuracy: 0.5 \n",
      "### rep 21  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 109.1 \n",
      "accuracy: 0.12 \n",
      "### rep 22  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 15.0 \n",
      "accuracy: 0.5 \n",
      "### rep 23  ###\n",
      "loss: 90.2 \n",
      "accuracy: 0.0 \n",
      "loss: 8.5 \n",
      "accuracy: 0.5 \n",
      "### rep 24  ###\n",
      "loss: 91.6 \n",
      "accuracy: 0.0 \n",
      "loss: 10.8 \n",
      "accuracy: 0.38 \n",
      "### rep 25  ###\n",
      "loss: 89.7 \n",
      "accuracy: 0.0 \n",
      "loss: 2.2 \n",
      "accuracy: 0.25 \n",
      "### rep 26  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 6.8 \n",
      "accuracy: 0.5 \n",
      "### rep 27  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.0 \n",
      "accuracy: 0.5 \n",
      "### rep 28  ###\n",
      "loss: 93.5 \n",
      "accuracy: 0.0 \n",
      "loss: 336.7 \n",
      "accuracy: 0.12 \n",
      "### rep 29  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 33.5 \n",
      "accuracy: 0.12 \n",
      "### rep 30  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 10.1 \n",
      "accuracy: 0.0 \n",
      "### rep 31  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.4 \n",
      "accuracy: 0.0 \n",
      "### rep 32  ###\n",
      "loss: 0.1 \n",
      "accuracy: 0.0 \n",
      "loss: 2.9 \n",
      "accuracy: 0.5 \n",
      "### rep 33  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.4 \n",
      "accuracy: 0.12 \n",
      "### rep 34  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.6 \n",
      "accuracy: 0.5 \n",
      "### rep 35  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.0 \n",
      "accuracy: 0.5 \n",
      "### rep 36  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 7.8 \n",
      "accuracy: 0.38 \n",
      "### rep 37  ###\n",
      "loss: 91.4 \n",
      "accuracy: 0.5 \n",
      "loss: 508.2 \n",
      "accuracy: 0.0 \n",
      "### rep 38  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.6 \n",
      "accuracy: 0.25 \n",
      "### rep 39  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.0 \n",
      "accuracy: 0.25 \n",
      "### rep 40  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 142.7 \n",
      "accuracy: 0.12 \n",
      "### rep 41  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 34.0 \n",
      "accuracy: 0.38 \n",
      "### rep 42  ###\n",
      "loss: 1.0 \n",
      "accuracy: 0.0 \n",
      "loss: 6.7 \n",
      "accuracy: 0.62 \n",
      "### rep 43  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.2 \n",
      "accuracy: 0.5 \n",
      "### rep 44  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.8 \n",
      "accuracy: 0.5 \n",
      "### rep 45  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 11.2 \n",
      "accuracy: 0.5 \n",
      "### rep 46  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.9 \n",
      "accuracy: 0.5 \n",
      "### rep 47  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 80.2 \n",
      "accuracy: 0.12 \n",
      "### rep 48  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.7 \n",
      "accuracy: 0.62 \n",
      "### rep 49  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.6 \n",
      "accuracy: 0.38 \n",
      "### rep 50  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.5 \n",
      "accuracy: 0.25 \n",
      "### rep 51  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 70.4 \n",
      "accuracy: 0.0 \n",
      "### rep 52  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.4 \n",
      "accuracy: 0.62 \n",
      "### rep 53  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.9 \n",
      "accuracy: 0.5 \n",
      "### rep 54  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.7 \n",
      "accuracy: 0.25 \n",
      "### rep 55  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 46.8 \n",
      "accuracy: 0.25 \n",
      "### rep 56  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 17.7 \n",
      "accuracy: 0.5 \n",
      "### rep 57  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.8 \n",
      "accuracy: 0.25 \n",
      "### rep 58  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.5 \n",
      "accuracy: 0.62 \n",
      "### rep 59  ###\n",
      "loss: 0.2 \n",
      "accuracy: 0.0 \n",
      "loss: 5.8 \n",
      "accuracy: 0.5 \n",
      "### rep 60  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.8 \n",
      "accuracy: 0.38 \n",
      "### rep 61  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 32.3 \n",
      "accuracy: 0.38 \n",
      "### rep 62  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.3 \n",
      "accuracy: 0.25 \n",
      "### rep 63  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 6.5 \n",
      "accuracy: 0.5 \n",
      "### rep 64  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.5 \n",
      "accuracy: 0.5 \n",
      "### rep 65  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 45.2 \n",
      "accuracy: 0.12 \n",
      "### rep 66  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.5 \n",
      "accuracy: 0.5 \n",
      "### rep 67  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.8 \n",
      "accuracy: 0.38 \n",
      "### rep 68  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.2 \n",
      "accuracy: 0.12 \n",
      "### rep 69  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 7.4 \n",
      "accuracy: 0.38 \n",
      "### rep 70  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 20.5 \n",
      "accuracy: 0.38 \n",
      "### rep 71  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.3 \n",
      "accuracy: 0.62 \n",
      "### rep 72  ###\n",
      "loss: 89.1 \n",
      "accuracy: 0.0 \n",
      "loss: 10.4 \n",
      "accuracy: 0.38 \n",
      "### rep 73  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.3 \n",
      "accuracy: 0.75 \n",
      "### rep 74  ###\n",
      "loss: 91.8 \n",
      "accuracy: 0.0 \n",
      "loss: 496.1 \n",
      "accuracy: 0.0 \n",
      "### rep 75  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 14.0 \n",
      "accuracy: 0.38 \n",
      "### rep 76  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.0 \n",
      "accuracy: 0.5 \n",
      "### rep 77  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.9 \n",
      "accuracy: 0.5 \n",
      "### rep 78  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.9 \n",
      "accuracy: 0.75 \n",
      "### rep 79  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 26.2 \n",
      "accuracy: 0.38 \n",
      "### rep 80  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.3 \n",
      "accuracy: 0.5 \n",
      "### rep 81  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.1 \n",
      "accuracy: 0.62 \n",
      "### rep 82  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.0 \n",
      "accuracy: 0.5 \n",
      "### rep 83  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.0 \n",
      "loss: 6.2 \n",
      "accuracy: 0.75 \n",
      "### rep 84  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.9 \n",
      "accuracy: 0.75 \n",
      "### rep 85  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 5.5 \n",
      "accuracy: 0.38 \n",
      "### rep 86  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 23.1 \n",
      "accuracy: 0.25 \n",
      "### rep 87  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.5 \n",
      "accuracy: 0.5 \n",
      "### rep 88  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 32.5 \n",
      "accuracy: 0.25 \n",
      "### rep 89  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.7 \n",
      "accuracy: 0.38 \n",
      "### rep 90  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.9 \n",
      "accuracy: 0.38 \n",
      "### rep 91  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 42.9 \n",
      "accuracy: 0.62 \n",
      "### rep 92  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 7.1 \n",
      "accuracy: 0.12 \n",
      "### rep 93  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 8.9 \n",
      "accuracy: 0.38 \n",
      "### rep 94  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 12.0 \n",
      "accuracy: 0.5 \n",
      "### rep 95  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 18.1 \n",
      "accuracy: 0.38 \n",
      "### rep 96  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.8 \n",
      "accuracy: 0.38 \n",
      "### rep 97  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.5 \n",
      "accuracy: 0.38 \n",
      "### rep 98  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 57.1 \n",
      "accuracy: 0.0 \n",
      "### rep 99  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.8 \n",
      "accuracy: 0.25 \n",
      "### rep 0  ###\n",
      "loss: 2.5 \n",
      "accuracy: 0.62 \n",
      "### rep 1  ###\n",
      "loss: 538.9 \n",
      "accuracy: 0.0 \n",
      "### rep 2  ###\n",
      "loss: 0.9 \n",
      "accuracy: 0.5 \n",
      "### rep 3  ###\n",
      "loss: 2.2 \n",
      "accuracy: 0.5 \n",
      "### rep 4  ###\n",
      "loss: 0.2 \n",
      "accuracy: 0.25 \n",
      "### rep 5  ###\n",
      "loss: 33.3 \n",
      "accuracy: 0.12 \n",
      "### rep 6  ###\n",
      "loss: 5.4 \n",
      "accuracy: 0.88 \n",
      "### rep 7  ###\n",
      "loss: 61.2 \n",
      "accuracy: 0.38 \n",
      "### rep 8  ###\n",
      "loss: 4.0 \n",
      "accuracy: 0.88 \n",
      "### rep 9  ###\n",
      "loss: 3.1 \n",
      "accuracy: 0.5 \n",
      "### rep 10  ###\n",
      "loss: 2.4 \n",
      "accuracy: 0.62 \n",
      "### rep 11  ###\n",
      "loss: 22.7 \n",
      "accuracy: 0.5 \n",
      "### rep 12  ###\n",
      "loss: 1.1 \n",
      "accuracy: 0.5 \n",
      "### rep 13  ###\n",
      "loss: 2.3 \n",
      "accuracy: 0.25 \n",
      "### rep 14  ###\n",
      "loss: 0.9 \n",
      "accuracy: 0.62 \n",
      "### rep 15  ###\n",
      "loss: 35.0 \n",
      "accuracy: 0.38 \n",
      "### rep 16  ###\n",
      "loss: 583.7 \n",
      "accuracy: 0.0 \n",
      "### rep 17  ###\n",
      "loss: 3.3 \n",
      "accuracy: 0.38 \n",
      "### rep 18  ###\n",
      "loss: 17.0 \n",
      "accuracy: 0.25 \n",
      "### rep 19  ###\n",
      "loss: 14.6 \n",
      "accuracy: 0.75 \n",
      "### rep 20  ###\n",
      "loss: 2.4 \n",
      "accuracy: 0.62 \n",
      "### rep 21  ###\n",
      "loss: 3.3 \n",
      "accuracy: 0.5 \n",
      "### rep 22  ###\n",
      "loss: 1.1 \n",
      "accuracy: 0.5 \n",
      "### rep 23  ###\n",
      "loss: 0.3 \n",
      "accuracy: 0.5 \n",
      "### rep 24  ###\n",
      "loss: 516.6 \n",
      "accuracy: 0.0 \n",
      "### rep 25  ###\n",
      "loss: 3.3 \n",
      "accuracy: 0.62 \n",
      "### rep 26  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.25 \n",
      "### rep 27  ###\n",
      "loss: 1.7 \n",
      "accuracy: 0.5 \n",
      "### rep 28  ###\n",
      "loss: 7.3 \n",
      "accuracy: 0.5 \n",
      "### rep 29  ###\n",
      "loss: 2.2 \n",
      "accuracy: 0.62 \n",
      "### rep 30  ###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.4 \n",
      "accuracy: 0.62 \n",
      "### rep 31  ###\n",
      "loss: 4.9 \n",
      "accuracy: 0.38 \n",
      "### rep 32  ###\n",
      "loss: 1.1 \n",
      "accuracy: 0.62 \n",
      "### rep 33  ###\n",
      "loss: 165.2 \n",
      "accuracy: 0.0 \n",
      "### rep 34  ###\n",
      "loss: 0.9 \n",
      "accuracy: 0.88 \n",
      "### rep 35  ###\n",
      "loss: 1.1 \n",
      "accuracy: 0.75 \n",
      "### rep 36  ###\n",
      "loss: 10.8 \n",
      "accuracy: 0.38 \n",
      "### rep 37  ###\n",
      "loss: 1.0 \n",
      "accuracy: 0.5 \n",
      "### rep 38  ###\n",
      "loss: 7.9 \n",
      "accuracy: 0.5 \n",
      "### rep 39  ###\n",
      "loss: 562.3 \n",
      "accuracy: 0.0 \n",
      "### rep 40  ###\n",
      "loss: 1.7 \n",
      "accuracy: 0.5 \n",
      "### rep 41  ###\n",
      "loss: 535.8 \n",
      "accuracy: 0.12 \n",
      "### rep 42  ###\n",
      "loss: 35.9 \n",
      "accuracy: 0.12 \n",
      "### rep 43  ###\n",
      "loss: 4.8 \n",
      "accuracy: 0.5 \n",
      "### rep 44  ###\n",
      "loss: 1.9 \n",
      "accuracy: 0.62 \n",
      "### rep 45  ###\n",
      "loss: 1.2 \n",
      "accuracy: 0.75 \n",
      "### rep 46  ###\n",
      "loss: 16.1 \n",
      "accuracy: 0.75 \n",
      "### rep 47  ###\n",
      "loss: 1.3 \n",
      "accuracy: 0.5 \n",
      "### rep 48  ###\n",
      "loss: 3.1 \n",
      "accuracy: 0.25 \n",
      "### rep 49  ###\n",
      "loss: 5.9 \n",
      "accuracy: 0.5 \n",
      "### rep 50  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.88 \n",
      "### rep 51  ###\n",
      "loss: 0.2 \n",
      "accuracy: 0.75 \n",
      "### rep 52  ###\n",
      "loss: 63.1 \n",
      "accuracy: 0.5 \n",
      "### rep 53  ###\n",
      "loss: 28.6 \n",
      "accuracy: 0.5 \n",
      "### rep 54  ###\n",
      "loss: 302.6 \n",
      "accuracy: 0.0 \n",
      "### rep 55  ###\n",
      "loss: 7.9 \n",
      "accuracy: 0.62 \n",
      "### rep 56  ###\n",
      "loss: 1.1 \n",
      "accuracy: 0.62 \n",
      "### rep 57  ###\n",
      "loss: 0.3 \n",
      "accuracy: 0.25 \n",
      "### rep 58  ###\n",
      "loss: 2.3 \n",
      "accuracy: 0.25 \n",
      "### rep 59  ###\n",
      "loss: 2.2 \n",
      "accuracy: 0.0 \n",
      "### rep 60  ###\n",
      "loss: 0.7 \n",
      "accuracy: 0.62 \n",
      "### rep 61  ###\n",
      "loss: 67.7 \n",
      "accuracy: 0.0 \n",
      "### rep 62  ###\n",
      "loss: 20.0 \n",
      "accuracy: 0.12 \n",
      "### rep 63  ###\n",
      "loss: 2.5 \n",
      "accuracy: 0.12 \n",
      "### rep 64  ###\n",
      "loss: 10.2 \n",
      "accuracy: 0.12 \n",
      "### rep 65  ###\n",
      "loss: 134.7 \n",
      "accuracy: 0.62 \n",
      "### rep 66  ###\n",
      "loss: 21.8 \n",
      "accuracy: 0.38 \n",
      "### rep 67  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.5 \n",
      "### rep 68  ###\n",
      "loss: 229.5 \n",
      "accuracy: 0.12 \n",
      "### rep 69  ###\n",
      "loss: 8.8 \n",
      "accuracy: 0.38 \n",
      "### rep 70  ###\n",
      "loss: 1.8 \n",
      "accuracy: 0.62 \n",
      "### rep 71  ###\n",
      "loss: 0.8 \n",
      "accuracy: 0.75 \n",
      "### rep 72  ###\n",
      "loss: 45.7 \n",
      "accuracy: 0.75 \n",
      "### rep 73  ###\n",
      "loss: 2.7 \n",
      "accuracy: 0.75 \n",
      "### rep 74  ###\n",
      "loss: 0.7 \n",
      "accuracy: 0.88 \n",
      "### rep 75  ###\n",
      "loss: 4.5 \n",
      "accuracy: 0.12 \n",
      "### rep 76  ###\n",
      "loss: 13.0 \n",
      "accuracy: 0.38 \n",
      "### rep 77  ###\n",
      "loss: 117.1 \n",
      "accuracy: 0.5 \n",
      "### rep 78  ###\n",
      "loss: 0.9 \n",
      "accuracy: 0.5 \n",
      "### rep 79  ###\n",
      "loss: 23.3 \n",
      "accuracy: 0.62 \n",
      "### rep 80  ###\n",
      "loss: 2.0 \n",
      "accuracy: 0.5 \n",
      "### rep 81  ###\n",
      "loss: 8.2 \n",
      "accuracy: 0.5 \n",
      "### rep 82  ###\n",
      "loss: 189.6 \n",
      "accuracy: 0.12 \n",
      "### rep 83  ###\n",
      "loss: 1.8 \n",
      "accuracy: 0.38 \n",
      "### rep 84  ###\n",
      "loss: 14.9 \n",
      "accuracy: 0.5 \n",
      "### rep 85  ###\n",
      "loss: 46.8 \n",
      "accuracy: 0.62 \n",
      "### rep 86  ###\n",
      "loss: 10.7 \n",
      "accuracy: 0.62 \n",
      "### rep 87  ###\n",
      "loss: 583.0 \n",
      "accuracy: 0.0 \n",
      "### rep 88  ###\n",
      "loss: 13.5 \n",
      "accuracy: 0.5 \n",
      "### rep 89  ###\n",
      "loss: 4.2 \n",
      "accuracy: 0.75 \n",
      "### rep 90  ###\n",
      "loss: 0.2 \n",
      "accuracy: 0.5 \n",
      "### rep 91  ###\n",
      "loss: 6.5 \n",
      "accuracy: 0.25 \n",
      "### rep 92  ###\n",
      "loss: 46.4 \n",
      "accuracy: 0.5 \n",
      "### rep 93  ###\n",
      "loss: 14.1 \n",
      "accuracy: 0.38 \n",
      "### rep 94  ###\n",
      "loss: 10.5 \n",
      "accuracy: 0.5 \n",
      "### rep 95  ###\n",
      "loss: 1.0 \n",
      "accuracy: 0.75 \n",
      "### rep 96  ###\n",
      "loss: 22.4 \n",
      "accuracy: 0.25 \n",
      "### rep 97  ###\n",
      "loss: 1.4 \n",
      "accuracy: 0.5 \n",
      "### rep 98  ###\n",
      "loss: 415.7 \n",
      "accuracy: 0.0 \n",
      "### rep 99  ###\n",
      "loss: 21.2 \n",
      "accuracy: 0.5 \n"
     ]
    }
   ],
   "source": [
    "# 2 step task\n",
    "\n",
    "num_sims = 100\n",
    "epochs = 200\n",
    "\n",
    "losses12 = []\n",
    "losses2 = []\n",
    "\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model, train_data1, test_data1, epochs)\n",
    "    model_copy = copy.deepcopy(model)\n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    losses12.append(loss1+loss2)\n",
    "    accs12.append(acc1+acc2)\n",
    "\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    losses2.append(loss2)\n",
    "    accs2.append(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "32fb7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "\n",
    "fileObject = open(save_dir + 'losses_12', 'wb')\n",
    "pickle.dump(losses2 , fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open(save_dir + 'losses_2', 'wb')\n",
    "pickle.dump(losses12 , fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "c07a1680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_12 = np.array(losses12).T\n",
    "losses_2 = np.array(losses2).T\n",
    "\n",
    "NNplt.plotNN_losses([losses_12],\\\n",
    "                    labels = ['1 2 step pretrained'],\\\n",
    "                    colors = ['orange'])\n",
    "\n",
    "NNplt.plotNN_shifted([losses_2],\\\n",
    "                     labels = ['2 step pretrained'],\\\n",
    "                     colors = ['green'],\\\n",
    "                     shift = 600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641b4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### rep 0  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.6 \n",
      "accuracy: 0.12 \n",
      "loss: 38.9 \n",
      "accuracy: 0.25 \n",
      "### rep 1  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 6.0 \n",
      "accuracy: 0.38 \n",
      "loss: 249.1 \n",
      "accuracy: 0.09 \n",
      "### rep 2  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.2 \n",
      "accuracy: 0.5 \n",
      "loss: 170.1 \n",
      "accuracy: 0.25 \n",
      "### rep 3  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.6 \n",
      "accuracy: 0.38 \n",
      "loss: 24.9 \n",
      "accuracy: 0.41 \n",
      "### rep 4  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 6.5 \n",
      "accuracy: 0.12 \n",
      "loss: 123.4 \n",
      "accuracy: 0.16 \n",
      "### rep 5  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.7 \n",
      "accuracy: 0.5 \n",
      "loss: 50.3 \n",
      "accuracy: 0.31 \n",
      "### rep 6  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.7 \n",
      "accuracy: 0.62 \n",
      "loss: 128.3 \n",
      "accuracy: 0.34 \n",
      "### rep 7  ###\n",
      "loss: 0.1 \n",
      "accuracy: 0.0 \n",
      "loss: 11.6 \n",
      "accuracy: 0.5 \n",
      "loss: 103.4 \n",
      "accuracy: 0.22 \n",
      "### rep 8  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 27.0 \n",
      "accuracy: 0.12 \n",
      "loss: 113.5 \n",
      "accuracy: 0.25 \n",
      "### rep 9  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 8.6 \n",
      "accuracy: 0.5 \n",
      "loss: 141.6 \n",
      "accuracy: 0.5 \n",
      "### rep 10  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 12.7 \n",
      "accuracy: 0.38 \n",
      "loss: 87.3 \n",
      "accuracy: 0.25 \n",
      "### rep 11  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.1 \n",
      "accuracy: 0.62 \n",
      "loss: 209.0 \n",
      "accuracy: 0.38 \n",
      "### rep 12  ###\n",
      "loss: 2.8 \n",
      "accuracy: 0.0 \n",
      "loss: 0.6 \n",
      "accuracy: 0.25 \n",
      "loss: 231.1 \n",
      "accuracy: 0.12 \n",
      "### rep 13  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 20.9 \n",
      "accuracy: 0.0 \n",
      "loss: 245.2 \n",
      "accuracy: 0.09 \n",
      "### rep 14  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.4 \n",
      "accuracy: 0.12 \n",
      "loss: 162.9 \n",
      "accuracy: 0.19 \n",
      "### rep 15  ###\n",
      "loss: 91.1 \n",
      "accuracy: 0.0 \n",
      "loss: 6.7 \n",
      "accuracy: 0.38 \n",
      "loss: 63.2 \n",
      "accuracy: 0.34 \n",
      "### rep 16  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.2 \n",
      "accuracy: 0.5 \n",
      "loss: 180.1 \n",
      "accuracy: 0.09 \n",
      "### rep 17  ###\n",
      "loss: 91.0 \n",
      "accuracy: 0.0 \n",
      "loss: 1.0 \n",
      "accuracy: 0.88 \n",
      "loss: 49.5 \n",
      "accuracy: 0.31 \n",
      "### rep 18  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.1 \n",
      "accuracy: 0.38 \n",
      "loss: 48.5 \n",
      "accuracy: 0.47 \n",
      "### rep 19  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.1 \n",
      "accuracy: 0.12 \n",
      "loss: 1068.7 \n",
      "accuracy: 0.06 \n",
      "### rep 20  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.4 \n",
      "accuracy: 0.25 \n",
      "loss: 79.5 \n",
      "accuracy: 0.19 \n",
      "### rep 21  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 4.7 \n",
      "accuracy: 0.5 \n",
      "loss: 2504.6 \n",
      "accuracy: 0.22 \n",
      "### rep 22  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.2 \n",
      "accuracy: 0.5 \n",
      "loss: 34.0 \n",
      "accuracy: 0.53 \n",
      "### rep 23  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.0 \n",
      "accuracy: 0.62 \n",
      "loss: 67.4 \n",
      "accuracy: 0.34 \n",
      "### rep 24  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.3 \n",
      "accuracy: 0.62 \n",
      "loss: 373.6 \n",
      "accuracy: 0.09 \n",
      "### rep 25  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.5 \n",
      "accuracy: 0.62 \n",
      "loss: 34.3 \n",
      "accuracy: 0.28 \n",
      "### rep 26  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.4 \n",
      "accuracy: 0.75 \n",
      "loss: 1208.3 \n",
      "accuracy: 0.38 \n",
      "### rep 27  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 10.9 \n",
      "accuracy: 0.38 \n",
      "loss: 146.9 \n",
      "accuracy: 0.41 \n",
      "### rep 28  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.9 \n",
      "accuracy: 0.25 \n",
      "loss: 294.2 \n",
      "accuracy: 0.03 \n",
      "### rep 29  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.0 \n",
      "accuracy: 0.62 \n",
      "loss: 71.7 \n",
      "accuracy: 0.47 \n",
      "### rep 30  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 17.0 \n",
      "accuracy: 0.12 \n",
      "loss: 412.1 \n",
      "accuracy: 0.09 \n",
      "### rep 31  ###\n",
      "loss: 83.2 \n",
      "accuracy: 0.0 \n",
      "loss: 2.3 \n",
      "accuracy: 0.5 \n",
      "loss: 54.9 \n",
      "accuracy: 0.25 \n",
      "### rep 32  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 10.6 \n",
      "accuracy: 0.5 \n",
      "loss: 152.9 \n",
      "accuracy: 0.25 \n",
      "### rep 33  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.5 \n",
      "accuracy: 0.12 \n",
      "loss: 64.9 \n",
      "accuracy: 0.34 \n",
      "### rep 34  ###\n",
      "loss: 0.4 \n",
      "accuracy: 0.0 \n",
      "loss: 4.3 \n",
      "accuracy: 0.75 \n",
      "loss: 105.9 \n",
      "accuracy: 0.28 \n",
      "### rep 35  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.2 \n",
      "accuracy: 0.0 \n",
      "loss: 151.1 \n",
      "accuracy: 0.19 \n",
      "### rep 36  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 3.4 \n",
      "accuracy: 0.62 \n",
      "loss: 153.7 \n",
      "accuracy: 0.31 \n",
      "### rep 37  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 2.5 \n",
      "accuracy: 0.5 \n",
      "loss: 427.3 \n",
      "accuracy: 0.41 \n",
      "### rep 38  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.1 \n",
      "accuracy: 0.38 \n",
      "loss: 125.4 \n",
      "accuracy: 0.19 \n",
      "### rep 39  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 9.3 \n",
      "accuracy: 0.38 \n",
      "loss: 41.3 \n",
      "accuracy: 0.53 \n",
      "### rep 40  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 0.6 \n",
      "accuracy: 0.25 \n",
      "loss: 214.8 \n",
      "accuracy: 0.09 \n",
      "### rep 41  ###\n",
      "loss: 0.0 \n",
      "accuracy: 0.0 \n",
      "loss: 16.3 \n",
      "accuracy: 0.38 \n",
      "loss: 39.2 \n",
      "accuracy: 0.28 \n",
      "### rep 42  ###\n",
      "loss: 97.3 \n",
      "accuracy: 0.5 \n",
      "loss: 7.2 \n",
      "accuracy: 0.5 \n"
     ]
    }
   ],
   "source": [
    "# 3 step task\n",
    "\n",
    "num_sims = 100\n",
    "epochs = 600\n",
    "\n",
    "losses123 = []\n",
    "accs123 = []\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss1, acc1 = run_acc(model, train_data1, test_data1, epochs)\n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    loss3, acc3 = run_acc(model, train_data3, test_data3, epochs)\n",
    "    losses123.append(loss1+loss2+loss3)\n",
    "    accs123.append(acc1+acc2+acc3)\n",
    "\n",
    "losses23 = []\n",
    "accs23 = []\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss2, acc2 = run_acc(model, train_data2, test_data2, epochs)\n",
    "    loss3, acc3 = run_acc(model, train_data3, test_data3, epochs)\n",
    "    losses23.append(loss2+loss3)\n",
    "    accs23.append(acc2+acc3)\n",
    "\n",
    "losses3 = []\n",
    "accs3 = []\n",
    "for j in range(num_sims):\n",
    "    print('### rep', j, ' ###')\n",
    "    model = OneStepRNN(input_size, output_size, hidden_size, num_layers)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "    loss3, acc3 = run_acc(model, train_data3, test_data3, epochs)\n",
    "    losses3.append(loss3)\n",
    "    accs3.append(acc3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436adeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Save\n",
    "\n",
    "fileObject = open(save_dir + 'losses_123', 'wb')\n",
    "pickle.dump(losses123 , fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open(save_dir + 'losses_23', 'wb')\n",
    "pickle.dump(losses23 , fileObject)\n",
    "fileObject.close()\n",
    "\n",
    "fileObject = open(save_dir + 'losses_3', 'wb')\n",
    "pickle.dump(losses3 , fileObject)\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc22492",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_123 = np.array(losses123).T\n",
    "losses_23 = np.array(losses23).T\n",
    "losses_3 = np.array(losses3).T\n",
    "\n",
    "NNplt.plotNN_losses([losses_123],\\\n",
    "                    labels = ['1 & 2 step pretrained'],\\\n",
    "                    colors = ['orange'])\n",
    "\n",
    "NNplt.plotNN_shifted([losses_23],\\\n",
    "                     labels = ['2 step pretrained'],\\\n",
    "                     colors = ['green'],\\\n",
    "                     shift = 600)\n",
    "\n",
    "NNplt.plotNN_shifted([losses_3],\\\n",
    "                     labels = ['random'],\\\n",
    "                     colors = ['blue'],\\\n",
    "                     shift = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffe970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bade2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 1.91 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#it -r 1 -n 1\n",
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f447bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
